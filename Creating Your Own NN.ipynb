{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def sgd_step(self, lrate): pass  # For modules w/o weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.dot(self.W.T, A) + self.W0  # (m x n)^T (m x b) = (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        self.dLdW = np.dot(self.A, dLdZ.T)                  # (m x n)\n",
    "        self.dLdW0 = dLdZ.sum(axis=1).reshape((self.n, 1))  # (n x 1)\n",
    "        return np.dot(self.W, dLdZ)                         # (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        self.W -= lrate*self.dLdW\n",
    "        self.W0 -= lrate*self.dLdW0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        return dLdA * (1.0 - (self.A ** 2))\n",
    "\n",
    "\n",
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.maximum(0, Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        return dLdA * (self.A != 0)\n",
    "\n",
    "\n",
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        return np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        return np.argmax(Ypred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        return float(np.sum(-Y * np.log(Ypred)))\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        return self.Ypred - self.Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        sum_loss = 0\n",
    "        for it in range(iters):\n",
    "            i = np.random.randint(N)\n",
    "            Xt = X[:, i:i+1]\n",
    "            Yt = Y[:, i:i+1]\n",
    "            Ypred = self.forward(Xt)\n",
    "            sum_loss += self.loss.forward(Ypred, Yt)\n",
    "            err = self.loss.backward()\n",
    "            self.backward(err)\n",
    "            self.sgd_step(lrate)\n",
    "\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints current loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print('Iteration =', it, '\tAcc =', acc, '\tLoss =', cur_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_simple_separable_through_origin():\n",
    "    X = np.array([[2, 3, 9, 12],\n",
    "                  [5, 1, 6, 5]])\n",
    "    y = np.array([[1, 0, 1, 0]])\n",
    "    return X, for_softmax(y)\n",
    "\n",
    "\n",
    "def super_simple_separable():\n",
    "    X = np.array([[2, 3, 9, 12],\n",
    "                  [5, 2, 6, 5]])\n",
    "    y = np.array([[1, 0, 1, 0]])\n",
    "    return X, for_softmax(y)\n",
    "\n",
    "\n",
    "def xor():\n",
    "    X = np.array([[1, 2, 1, 2],\n",
    "                  [1, 2, 2, 1]])\n",
    "    y = np.array([[1, 1, 0, 0]])\n",
    "    return X, for_softmax(y)\n",
    "\n",
    "\n",
    "def xor_more():\n",
    "    X = np.array([[1, 2, 1, 2, 2, 4, 1, 3],\n",
    "                  [1, 2, 2, 1, 3, 1, 3, 3]])\n",
    "    y = np.array([[1, 1, 0, 0, 1, 1, 0, 0]])\n",
    "    return X, for_softmax(y)\n",
    "\n",
    "\n",
    "def hard():\n",
    "    X = np.array([[-0.23390341, 1.18151883, -2.46493986, 1.55322202, 1.27621763,\n",
    "                   2.39710997, -1.3440304, -0.46903436, -0.64673502, -1.44029872,\n",
    "                   -1.37537243, 1.05994811, -0.93311512, 1.02735575, -0.84138778,\n",
    "                   -2.22585412, -0.42591102, 1.03561105, 0.91125595, -2.26550369],\n",
    "                  [-0.92254932, -1.1030963, -2.41956036, -1.15509002, -1.04805327,\n",
    "                   0.08717325, 0.8184725, -0.75171045, 0.60664705, 0.80410947,\n",
    "                   -0.11600488, 1.03747218, -0.67210575, 0.99944446, -0.65559838,\n",
    "                   -0.40744784, -0.58367642, 1.0597278, -0.95991874, -1.41720255]])\n",
    "    y = np.array([[1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
    "                   1., 0., 0., 0., 1., 1., 0.]])\n",
    "    return X, for_softmax(y)\n",
    "\n",
    "\n",
    "def for_softmax(y):\n",
    "    return np.vstack([1 - y, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_tanh_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_relu_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_pred_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    Ypred = nn.forward(X)\n",
    "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n",
    "\n",
    "def nn_hard_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10,2), SoftMax()], NLL())\n",
    "    X, Y = hard()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    Ypred = nn.forward(X)\n",
    "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
       "   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
       "   [-8.47337764291184e-12, 2.6227368810847102e-09, 0.00017353185263155828]],\n",
       "  [[0.544808855557535, -0.08366117689965663],\n",
       "   [-0.06331837550937103, 0.24078409926389266],\n",
       "   [0.08677202043839037, 0.8360167748667923],\n",
       "   [-0.0037249480614717995, 0.0037249480614718]]],\n",
       " [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
       "   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
       "   [-0.002775491757223511, 0.001212351486908601, -0.0005239629389906042]],\n",
       "  [[0.501769700845158, -0.04062202218727964],\n",
       "   [-0.09260786974986725, 0.27007359350438886],\n",
       "   [0.08364438851530624, 0.8391444067898763],\n",
       "   [-0.004252310922204505, 0.004252310922204505]]],\n",
       " ([0, 0, 0, 0], [8.565750618357669]),\n",
       " ([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0],\n",
       "  [13.615265848172868]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_tanh_test(),nn_relu_test(),nn_pred_test(),nn_hard_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
